{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Motivation\n",
    "\n",
    "- Aircraft Manufacturer Example:\n",
    "    - Dataset {$x^{(1)},x^{(2)},...,x^{(m)}$}\n",
    "        - $x_1$ = heat generated\n",
    "        - $x_2$ = vibration intensity\n",
    "    - Anomaly detection takes new data $x_{test}$ and compares it to the training set to determine if there are any major fluctuations or anomalies in the output performance.\n",
    "    \n",
    "- If we have a model $p(x)$, then we run our new data through the model, and if $p(x_{test}) < \\epsilon$ then we flag it as an anomaly\n",
    "\n",
    "- Aother example is Fraud Detection:\n",
    "    - $x^{(i)}$ = features of user i's activities\n",
    "    - Model $p(x)$ from data\n",
    "    - Identify unusual users by checking which have $p(x) < \\epsilon$\n",
    "\n",
    "### Gaussian (Normal) Distribution\n",
    "\n",
    "- Say $x \\in \\mathbb{R}$. If x is a distributed Gaussian with mean $\\mu$ and variance $\\sigma^2$ \n",
    "    - $x$ **~** $N(\\mu, \\sigma^2)$\n",
    "    - ~ means 'distributed as'\n",
    "    - plotting the Gaussian distribution will result in the bell curve\n",
    "- $p(x;\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi}*\\sigma}exp(-\\frac{(x- \\mu)^2}{2 \\sigma ^2})$\n",
    "    - $\\sigma^2$ = variance\n",
    "    - $\\sigma$ = standard deviation\n",
    "    \n",
    "- Parameter estimation:\n",
    "    - $\\mu = \\frac{1}{m} \\sum \\limits_{i=1}^m x^{(i)}$ \n",
    "    - $\\sigma^2 = \\frac{1}{m} \\sum \\limits_{i=1}^m (x^{(i)} - \\mu)^2$\n",
    "    \n",
    "### Anomaly Detection Algorithm\n",
    "\n",
    "- $p(x) = p(x_1;\\mu,\\sigma^2),p(x_2;\\mu,\\sigma^2),p(x_3;\\mu,\\sigma^2),...p(x_n;\\mu,\\sigma^2)$\n",
    "    - $p(x) = \\prod \\limits_{j=1}^n p(x_j;\\mu_j,\\sigma_j^2)$\n",
    "    - Also called the **Density Estimation**\n",
    "- Anomaly Detection Algorithm:\n",
    "    1. Chose features $x_i$ that might be indicative of anomalous examples\n",
    "    2. Fit parameters $\\mu_1,...\\mu_n,\\sigma_1^2,...\\sigma_n^2$\n",
    "    3. Given new example x, compute $p(x)$, and $p(x)$ is an anomaly if < $\\epsilon$\n",
    "    \n",
    "### Building and Developing an Anomaly Detection System\n",
    "\n",
    "- Always easier to evaluate a model if we can return some real-number 'score' to determine how well a model performs when adding or removing certain features\n",
    "    - For anomaly detection systems, assume we have some labeled data of anomalous and non-anomalous data example (i.e. y = 0 if normal and y = 1 if anomalous)\n",
    "    - Define training (assume normal/not anomalous), cross-validation, and testing sets\n",
    "    - Example:\n",
    "        - 10000 good aircraft engines (y=0) and 20 anomalous engines (y=1)\n",
    "            - Training Set: 6000 good engines (Train $p(x) = p(x_1;\\mu_1,\\sigma_1^2),p(x_2;\\mu_2,\\sigma_2^2),...p(x_n;\\mu_n,\\sigma_n^2)$\n",
    "            - CV: 2000 Good engines and 10 anomalous\n",
    "            - Test: 2000 Good engines and 10 anomalous\n",
    "    - Possible evaluation metrics:\n",
    "        1. True positive, false positive, false negative, true negative\n",
    "        2. Precision/Recall\n",
    "        3. $F_1$ score\n",
    "    - Choose different values of the threshold parameter $\\epsilon$ that maximizes the $F_1$ score\n",
    "    \n",
    "### Choosing Which Features to Use\n",
    "\n",
    "- First step is to plot data to check and see if it has a vaguely Gaussian distribution (histogram/density graph)\n",
    "    - if data does **not** look Gaussian, you can play around with some different transformations to manipulate the data shape:\n",
    "        - $log(x)$\n",
    "        - $log(x+c)$\n",
    "        - $\\sqrt{x}$\n",
    "        - $x^d$\n",
    "- Error analysis for anomaly detection:\n",
    "    - Want $p(x)$ to be large for normal examples x and $p(x)$ to be small for anomalous examples x\n",
    "    - A common problem would be that $p(x)$ is comparable (e.g. both are large) for normal and anomalous examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Distribution\n",
    "\n",
    "- $x \\in \\mathbb{R}^n$ Don't model $p(x_1),p(x_2),...,$ etc. separately, but instead model $p(x)$ all in one go\n",
    "    - Parameters: $\\mu \\in \\mathbb{R}^n, \\Sigma \\in \\mathbb{R}^{n x n}$ (Covariance matric)\n",
    "    - $p(x;\\mu,\\sigma^2) = \\frac{1}{2\\pi^{\\frac{n}{2}}\\mid \\Sigma \\mid^{\\frac{1}{2}}} exp(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu))$\n",
    "        - $\\mid \\Sigma \\mid$ = the detminant of a matrix\n",
    "        \n",
    "- Parameter fitting:\n",
    "    - Given a training set {$x^{(1)},x^{(2)},...,x^{(m)}$}\n",
    "        - $\\mu = \\frac{1}{m}\\sum \\limits_{i=1}^m x^{(i)}$\n",
    "        - $\\Sigma = \\frac{1}{m}\\sum \\limits_{i=1}^m (x^{(i)}-\\mu)(x^{(i)}-\\mu)^T$\n",
    "- Anomaly detection with the Multivariate Gaussian:\n",
    "    1. Fit model $p(x)$ by setting\n",
    "        - $\\mu = \\frac{1}{m}\\sum \\limits_{i=1}^m x^{(i)}$\n",
    "        - $\\Sigma = \\frac{1}{m}\\sum \\limits_{i=1}^m (x^{(i)}-\\mu)(x^{(i)}-\\mu)^T$\n",
    "    2.  Given a new example x, compute\n",
    "        - $p(x;\\mu,\\sigma^2) = \\frac{1}{2\\pi^{\\frac{n}{2}}\\mid \\Sigma \\mid^{\\frac{1}{2}}} exp(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu))$\n",
    "    3. Flag an anomaly if $p(x)< \\epsilon$\n",
    "    \n",
    "- Original Model Vs. Multivariate Gaussian:\n",
    "    - Original model:\n",
    "        - most common\n",
    "        - manually create new features to capture anomalies where $x_1,x_2$ take unusual combinations of values\n",
    "        - computationally cheaper (scales better to large n)\n",
    "        - okay if training size m is small\n",
    "    - Multivariate model:\n",
    "        - captures correlations between features\n",
    "        - automatically captures correlations between features\n",
    "        - computationally more expensive\n",
    "        - must have m > n or $\\Sigma$ is non-invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "### Content-based recommender systems\n",
    "- For each user j, learn a parameter $\\theta^{(j)} \\in \\mathbb{R}^{n+1}$ (n = 2, the number of features). Predict user j as rating movie i with $(\\theta^{(j)})^T x^{(i)}$ stars\n",
    "    - each user will have a different parameter vector associated with them\n",
    "- Problem Formulation:\n",
    "    - $r(i,j)$ = 1 if user j has rated movie i (0 otherwise)\n",
    "    - $y^{(i,j)}$ = rating by user j on movie i (if defined)\n",
    "    \n",
    "    - $\\theta^{(j)}$ = parameter vector for user j\n",
    "    - $x^{(i)}$ = feature vector for movie i\n",
    "   \n",
    "    - for user j, movie i, predict rating: $(\\theta^{(j)})^T x^{(i)}$\n",
    "    - $m^{(j)}$ = number of movies rated by user j\n",
    "    - To learn $\\theta^{(j)}$:\n",
    "        1. min $\\theta^{(j)}$ = $ \\frac{1}{2m^{(j)}}\\sum \\limits_{i:r(i,j) = 1} ((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})^2 + \\frac{\\lambda}{2m^{(j)}}\\sum \\limits_{k=1}^n (\\theta_k^{(j)})^2$\n",
    "            - can also be written as: $\\theta^{(j)}$ = $ \\frac{1}{2}\\sum \\limits_{i:r(i,j) = 1} ((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum \\limits_{k=1}^n (\\theta_k^{(j)})^2$\n",
    "        2. However, for a recommender system, you want to learn parameters for **all** the users, not just a single user:\n",
    "            - $\\theta^{(1)},...,\\theta^{(n_u)}$ = $ \\frac{1}{2}\\sum \\limits_{j=1}^{n_u} \\sum \\limits_{i:r(i,j) = 1} ((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum \\limits_{j=1}^{n_u} \\sum \\limits_{k=1}^n (\\theta_k^{(j)})^2$\n",
    "        3. Gradient Descent Update:\n",
    "            - $\\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha \\sum \\limits_{i:r(i,j) = 1}((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})x_k^{(i)}$ (for k = 0)\n",
    "            - $\\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha (\\sum \\limits_{i:r(i,j) = 1}((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})x_k^{(i)}+ \\lambda \\theta_k^{(j)})$ (when $k \\neq 0$)\n",
    "            \n",
    "            \n",
    "### Collaborative Filtering\n",
    "\n",
    "- Given $\\theta^{(1)},...,\\theta^{(n_m)}$ where $n_m$ = the number of movies,\n",
    "    - min $x^{(1)},...,x^{(n_m)}$ $\\frac{1}{2}\\sum \\limits_{i=1}^{n_m} \\sum \\limits_{j:r(i,j) = 1} ((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum \\limits_{i=1}^{n_m} \\sum \\limits_{k=1}^n (x_k^{(i)})^2$ for all movies to determine features\n",
    "    \n",
    "- Collaborative Filtering Process:\n",
    "    - Given $x^{(1)},...,x^{(n_m)}$ (and moving ratings), estimate $\\theta^{(1)},...,\\theta^{(n_u)}$\n",
    "    - Given $\\theta^{(1)},...,\\theta^{(n_u)}$, estimate $x^{(1)},...,x^{(n_m)}$\n",
    "    \n",
    "    - Iterate back and forth between estimating $\\theta$ parameters and $x$ features\n",
    "    \n",
    " ### Collaborative Filtering Algorithm\n",
    " \n",
    " - The above cost functions for minimize wrt $\\theta$ and $x$ can be combined into a single function to create a more efficient algorithm: $J(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)}) = \\frac{1}{2}\\sum \\limits_{(i,j):r(i,j) = 1}((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum \\limits_{i=1}^{n_m} \\sum \\limits_{k=1}^n (x_k^{(i)})^2 + \\frac{\\lambda}{2}\\sum \\limits_{j=1}^{n_u} \\sum \\limits_{k=1}^n (\\theta_k^{(j)})^2$\n",
    " - Because we are learning the features automatically with the algorithm, we no longer need the covention $x_0 = 1$, so $x \\in \\mathbb{R}^n$ and $\\theta \\in \\mathbb{R}^n$\n",
    " \n",
    " - Stepwise Process:\n",
    "     1. Intialize $x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)}$ to small random values\n",
    "     2. Minimuze $J(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})$ using gradient descent (or an advanced optimziation algorithm) E.g. for every $j=1,...,n_u, i=1,...,n_m$:\n",
    "         - $x_k^{(i)} := x_k^{(i)} - \\alpha (\\sum \\limits_{j:r(i,j) = 1}((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})\\theta_k^{(j)}+ \\lambda x_k^{(i)})$\n",
    "         - $\\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha (\\sum \\limits_{i:r(i,j) = 1}((\\theta^{(j)})^T x^{(i)}-y^{(i,j)})x_k^{(i)}+ \\lambda \\theta_k^{(j)})$\n",
    "     3. For a user with parameters $\\theta$ and a movie with learned features $x$, predict a start rating of $\\theta^Tx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
