{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Features\n",
    "\n",
    "Consider the housing price example from week 1. In that example, there was one variable, x, which represented the size in sq ft of the house. The x variable was used to predict the price (y) of the home. What if there were other variables contributing to the price of the house (e.g. # of bedrooms, # of floors, and the age of the home in years).\n",
    "\n",
    "Notation:\n",
    "- $n$ = number of features\n",
    "- $x^{(i)}$  = input (features) of the $i^{th}$ training example. For example, $x^{(2)} = \\begin{bmatrix} 1416 \\\\ 3 \\\\ 2 \\\\ 40 \\end{bmatrix}$ Which is a vector of feature values for the 2nd training example.\n",
    "- $x^{(i)}_j$ = value of the feature *j* of the $i^{th}$ training example.\n",
    "\n",
    "What is the new hypothesis for multiple features?\n",
    "- the previous hypothesis for univariant linear regression was $ h_\\mathsf{\\theta}(x) = \\mathsf{\\theta_0}+\\mathsf{\\theta_1x}$\n",
    "- $ h_\\mathsf{\\theta}(x)=\\mathsf{\\theta_0}+\\mathsf{\\theta_1x_1}+\\mathsf{\\theta_2x_2}+\\mathsf{\\theta_3x_3}+\\mathsf{\\theta_4x_4}$\n",
    "- The above formula represents the summation of n features. In the housing example, there are 4 features. it can more formally/generally be written as $h_\\mathsf{\\theta}(x)=\\mathsf{\\theta_0}+\\mathsf{\\theta_1x_1}+\\mathsf{\\theta_2x_2}+...+\\mathsf{\\theta_nx_n}$\n",
    "    - for convenience of notation, $x_0 = 1$ or $x^{(i)}_0 = 1$\n",
    "    - $x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$ x is therefore a 0-index feature vector.\n",
    "    - $\\mathsf{\\theta} = \\begin{bmatrix} \\mathsf{\\theta_0} \\\\ \\mathsf{\\theta_1} \\\\ \\mathsf{\\theta_2} \\\\ \\mathsf{\\theta_3} \\end{bmatrix}$ or the parameters, can also be written as a vector\n",
    "- $ h_\\mathsf{\\theta}(x)= \\mathsf{\\theta^{T}x}$\n",
    "    - $\\begin{bmatrix}\\mathsf{\\theta_0} &\\mathsf{\\theta_1} &\\mathsf{\\theta_n}\\end{bmatrix}\\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_n \\end{bmatrix} = h_\\mathsf{\\theta}(x)=\\mathsf{\\theta_0x_0}+\\mathsf{\\theta_1x_1}+\\mathsf{\\theta_2x_2}+\\mathsf{\\theta_nx_n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Multiple Variables\n",
    "\n",
    "- Think of the parameters $\\mathsf{\\theta_0},\\mathsf{\\theta_1},...\\mathsf{\\theta_n}$ as $\\mathsf{\\theta}$ = a n+1 dimensional vector\n",
    "- Cost function:\n",
    "J($\\mathsf{\\theta}_0$ $\\mathsf{\\theta}_1$...$\\mathsf{\\theta}_n$) =  $\\frac{1}{2m}$ $\\sum \\limits_{i=1} ^{m}(h_\\mathsf{\\theta}(x^i)-y^i)^2$\n",
    "    - instead of writing J($\\mathsf{\\theta}_0$ $\\mathsf{\\theta}_1$...$\\mathsf{\\theta}_n$), J($\\mathsf{\\theta}$) = a function of the parameter vector theta\n",
    "- Gradient descent:\n",
    "    - Repeat $\\mathsf{\\theta}_j := \\mathsf{\\theta}_j -\\mathsf{\\alpha}\\frac{\\mathsf{\\delta}}{\\mathsf{\\delta}\\mathsf{\\theta}_j}J(\\mathsf{\\theta})$\n",
    "    - $\\mathsf{\\theta}_j := \\mathsf{\\theta}_j -\\mathsf{\\alpha}\\frac{1}{m}\\sum \\limits_{i=1}^{m}(h_\\mathsf{\\theta_0}(x^{(i)})-y^{(i)})x^{(i)}_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Practice I: Feature Scaling\n",
    "- If you make sure that the various features are on a similar scale, then convergence via gradient descent can occure more quickly.\n",
    "- E.g. $x_1 = size(0-2000ft^2)$ and $x_2 =bedrooms(1-5)$\n",
    "    - the contours of the associated graph would be skewed, i.e. very tall and thin ovals because $x_1=2000$ vs $x_2=5$ represent a large difference in scale.\n",
    "    - one solution would be to scale the feature values:\n",
    "        - $x_1 = \\frac{size(ft^2)}{2000}$\n",
    "        - $x_2 = \\frac{number of bedrooms}{5}$\n",
    "- Ideally the idea is to get every feature into approximately a $ -1 ≤ x_i ≤ 1$ range.\n",
    "- Mean normalization:\n",
    "    - replace $x_i$ with $x_i - \\mathsf{\\mu_i}$ to make features have approximately zero mean\n",
    "    - e.g. $x_1 = \\frac{size-1000}{2000}$ $x_2=\\frac{numberofbedrooms-2}{5}$\n",
    "    - $x_1 = \\frac{x_1 - \\mathsf{\\mu_1}}{s_1}$\n",
    "    -  $\\mathsf{\\mu_i}$ = average value of $x_i$\n",
    "    - $s_1$ = the range of the values, i.e. the max value - the min value\n",
    "        - $s_1$ can also be replaced with the standard deviation value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Practice II: Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
