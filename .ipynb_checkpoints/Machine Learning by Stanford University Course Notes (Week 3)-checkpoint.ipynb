{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "- Examples of classification problems in ML:\n",
    "    - Email: Spam/Not Spam?\n",
    "    - Online Transactions: Fraudulent (Yes/No)?\n",
    "    - Tumor: Malignant/Benign?\n",
    "- $y\\in$ {0,1}\n",
    "    - 0: Negative Class\n",
    "    - 1: Positive Class\n",
    "- If using linear regression ($h_\\mathsf{\\theta}(x) = \\mathsf{\\theta^T}x$), a threshold classifier output $h_\\mathsf{\\theta}(x)$ at 0.5 would result in something like this:\n",
    "    - if $h_\\mathsf{\\theta}(x)\\geq$ 0.5, predict \"y=1\".\n",
    "    - if $h_\\mathsf{\\theta}(x)\\leq$ 0.5, predict \"y=0\".\n",
    "    - However, because training set data might have outliers that cause the threshold to shift and ultimately misclassify some data points, linear regression is usually not a great idea for classification problems.\n",
    "    \n",
    "- Logistic Regression: $0\\leq h_\\mathsf{\\theta}(x) \\leq 1$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Representation for Logistic Regression\n",
    "\n",
    "- $h_\\mathsf{\\theta}(x) = g(\\mathsf{\\theta^T}x)$\n",
    "    - $g(z)=\\frac{1}{1+e^-z}$\n",
    "    - $g(z)$ is the sigmoid/logistic function\n",
    "    - an alternate form of the hypothesis is: $h_\\mathsf{\\theta}(x) = \\frac{1}{1+e^{-{\\mathsf{\\theta^T}x}}}$\n",
    "- Interpretation of Hypothesis Output:\n",
    "    - $h_\\mathsf{\\theta}(x)$ = estimated probability that y = 1 on input x\n",
    "    - Example:\n",
    "        - if $ x = \\begin{bmatrix}x_0 \\\\ x_1\\end{bmatrix}$ = $\\begin{bmatrix}1 \\\\ tumorSize\\end{bmatrix}$\n",
    "        - $h_\\mathsf{\\theta}(x) = 0.7$, You would interpret the results as a 70% chance that the tumor is malignant.\n",
    "    - Formally, this probabilty is expressed as: $h_\\mathsf{\\theta}(x) = P(y=1|x;\\mathsf{\\theta})$, which should be read as 'the probability that y=1, given x, parameterized by $\\mathsf{\\theta}$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "\n",
    "- Suppose predict \"y=1\" if $h_\\mathsf{\\theta}(x) \\geq 0.5$ and predict \"y=0\" if $h_\\mathsf{\\theta}(x) < 0.5$\n",
    "- If we visualize the sigmoid function, then $g(z) \\geq 0.5$ when $z \\geq 0$. Therefore, $h_\\mathsf{\\theta}(x) = g(\\mathsf{\\theta^T}x) \\geq 0.5$ whenever $\\mathsf{\\theta^T}x \\geq 0$\n",
    "    - the oppositve would be true whenever $g(\\mathsf{\\theta^T}x) < 0.5$\n",
    "- A training set may be used to fit the parameters, but the decision boundary itself is a property of the paramter vector $\\mathsf{\\theta}$, **not** the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
