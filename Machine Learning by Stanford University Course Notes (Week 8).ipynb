{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Unsupervised Learning\n",
    "\n",
    "- An unsupervised learning problem consists of data without any labels associated with it. I.e. A set of data with no positve or negative labels, just a training set of $x^{(1)}, x^{(2)}, x^{(3)},..., x^{(m)}$ values (a training set with no y values corresponding to each x value)\n",
    "- An unsupervised learning algorithm is used to find patterns/structure in an unlabled dataset\n",
    "    - an algorithm that determines/discovers data grouped into unique clusters is called a clustering algorithm\n",
    "    - Applications of clustering:\n",
    "        - Market segmentation\n",
    "        - Social network analysis\n",
    "        - Organize computing clusters\n",
    "        - Astronomical data analysis\n",
    "        \n",
    "### K-means Algorithm\n",
    "\n",
    "1. When running the K-means algorithm, the first step is to randomly initialize the **cluster centroids**. The number of cluster centroids, K is equaled to the number of clusters you wish to try to seperate your data into.\n",
    "2. The next step is the cluster assignment step. Each training set will be assigned a cluster centroid based on its proxmity to the centroid.\n",
    "3. The move centroid step moves the centroid to the average of the points assigned to the same centroid.\n",
    "4. Repeat steps 2 - 3 until convergence\n",
    "\n",
    "- Formal definition of K-means algorithm\n",
    "    - Input:\n",
    "        - K (number of clusters)\n",
    "        - Training set {$x^{(1)}, x^{(2)}, ..., x^{(m)}$}\n",
    "        \n",
    "    - $x^{(i)} \\in \\mathbb{R}^n$ (drop $x_0 = 1$ covnention)\n",
    "    \n",
    "    - Randomly initialize K cluster centroids $\\mu_1,\\mu_1,...,\\mu_K \\in \\mathbb{R}^n$\n",
    "    - Repeat:\n",
    "        - For i = 1 to m\n",
    "            - $c^{(i)}:=$ index (from 1 to K) cluster centroid closest to $x^{(i)}$\n",
    "            - also written as $c^{(i)}:= \\left\\Vert x^{(i)} - \\mu_k \\right\\Vert^2$ = the distance between the xi training example and the cluster centroid. Find the min value of k that minimizes the distances function and assign xi to that cluster centroid k.\n",
    "        - For k = 1 to K\n",
    "            - $\\mu_k:=$ average (mean) of points assigned to cluster k\n",
    "            \n",
    "### Optimization Objective\n",
    "\n",
    "- Notation:\n",
    "    - $c^{(i)}=$ index of cluster to which example $x^{(i)}$ is currently assigned\n",
    "    - $\\mu_k=$ cluster centroid $k$\n",
    "    - $\\mu_{c^{(i)}}=$ cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n",
    "    \n",
    "- Optimization Objective:\n",
    "    - $J(c^{(1)},...,c^{(m)},\\mu_1,...\\mu_K) = \\frac{1}{m}\\sum \\limits_{i=1}^m\\left\\Vert x^{(i)} - \\mu_{c^{(i)}} \\right\\Vert^2$\n",
    "    - Also called the Distortion Cost Function, or the Distortion of the k-means algorithm\n",
    "    \n",
    "### Random Initialization\n",
    "\n",
    "- Randomly choose K training examples (K = number of desired cluster centroids) and set $\\mu_1,...\\mu_k$ equal to those K examples.\n",
    "- K-means can sometimes converge to local optima depending on the where the randomly initialized cluster centroids are assigned\n",
    "    - Try multiple random initializations and select the best result to avoid k-means getting stuck at a local optima\n",
    "    - Example:\n",
    "        - For i = 1 to 100:\n",
    "            1. Randomly initialize K-means\n",
    "            2. Run K-means. Get $(c^{(1)},...,c^{(m)},\\mu_1,...\\mu_K)$\n",
    "            3. Compute cost function (distortion)\n",
    "         - Select the one with the lowest cost $J(c^{(1)},...,c^{(m)},\\mu_1,...\\mu_K)$\n",
    "         \n",
    "### Choosing the Number of Clusters\n",
    "\n",
    "- Elbow Method:\n",
    "    - Plot the cost function J with respect to the number of K clusters. The 'elbow' in the cost function curve represents the K number of clusters to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation I: Data Compression\n",
    "\n",
    "- Reduce data from 2D to 1D\n",
    "    - Example where 2 features of a dataset are $x_1=cm$ and $x_2=in$. Both features measure essentially the same thing and can be reduced to a single 1D feature to avoid redudancy in the dataset\n",
    "    - $x^{(m)} \\in \\mathbb{R}^2$ --> $z^{(m)}\\in \\mathbb{R}$\n",
    "- Reduce data from 3D to 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis Overview (PCA)\n",
    "\n",
    "- PCA tries to find a line onto which the data can be projected so that the sum of squares (a.k.a the projected error) is minimized.\n",
    "- Any feature normalization is conducted prior to applying PCA\n",
    "- Formal Definition: \n",
    "    - Reduce from 2d to 1d = Find a direction (a vector $u^{(i)} \\in \\mathbb{R}$) onto which to project the data so as to minimize the projection error\n",
    "    - Reduce from n-dimensional to k-dimensional = Find k vectors $u^{(1)},u^{(2)},...,u^{(k)}$ onto which to project the data so as to minimize the projection error\n",
    "    \n",
    "### PCA Algorithm\n",
    "\n",
    "- Data preprocessing:\n",
    "    - Training set: $x^{(1)},x^{(2)},...,x^{(m)}$\n",
    "    - Preprocessing (feature scaling/mean normalization):\n",
    "        - $\\mu_j = \\frac{1}{m} \\sum \\limits_{i=1}^m x_j^{(i)}$ Replace $x_j^{(i)}$ with $x_j - \\mu_j$\n",
    "        - If different features on different scales, scale features to have comparable range of values\n",
    "        \n",
    "- PCA Procedure:\n",
    "    - Reduce data from n-dimensions to k-dimensions:\n",
    "        - Compute **covariance matrix**\n",
    "            - $\\Sigma = \\frac{1}{m}\\sum \\limits_{i=1}^n(x^{(i)})(x^{(i)})^T$\n",
    "        - Compute **eigenvectors** of matrix $\\Sigma$\n",
    "            - Octave function: [U,S,V] = svd(Sigma);\n",
    "                - svd = Singular Value Decomposition\n",
    "                - eig(Sigma) will also work\n",
    "        - The columns of the U ($U \\in \\mathbb{R}^{nxn}$) matrix will equal $u^{(1)},u^{(2)},u^{(3)},...,u^{(m)}$\n",
    "        - The first k ($u^{(1)}-u^{(k)}$)columns of U are the vectors used to define the k-dimensional reduction\n",
    "            - $U_{reduce}$ = n x k matrix\n",
    "            - $z = U_{reduce}^T x$\n",
    "            \n",
    "### Reconstruction from Compressed Representation\n",
    "\n",
    "- $x_{approx} = U_{reduce} z$\n",
    "\n",
    "### Choosing the Number of Principal Components\n",
    "\n",
    "- Average Square projection error:$\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)} - x_{approx} \\right\\Vert^2$\n",
    "- Total variation in the data: $\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)}\\right\\Vert^2$\n",
    "- Typically choose k to be the smallest value so that \n",
    "    - $\\frac{\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)} - x_{approx} \\right\\Vert^2}{\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)}\\right\\Vert^2} \\leq 0.01$ (1%)\n",
    "    - 99% of variance is retained\n",
    "- Other common \"thresholds\" are 5% and 10% (i.e. 95% and 90% variance retained)\n",
    "\n",
    "- Algorithm for choosing k:\n",
    "    - Try PCA with k = 1\n",
    "    - Compute $U_{reduce}, z^{(1)},z^{(2)},...,z^{(m)},x_{approx}^{(1)},x_{approx}^{(2)},...,x_{approx}^{(m)}$\n",
    "    - Check if $\\frac{\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)} - x_{approx} \\right\\Vert^2}{\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)}\\right\\Vert^2} \\leq 0.01$\n",
    "    - Repeat until above is true\n",
    "    - However this is inefficient. The Octave function [U,S,V] = svd(Sigma) returns an n x n matrix S in which the diagonal are the only non zero items in the matrix.\n",
    "        - For a given k:\n",
    "            - $ 1 - \\frac{\\sum \\limits_{i=1}^k S_{ii}}{\\sum \\limits_{i=1}^n S_{ii}}$ = $\\frac{\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)} - x_{approx} \\right\\Vert^2}{\\frac{1}{m} \\sum \\limits_{i=1}^m \\left\\Vert x^{(i)}\\right\\Vert^2}$\n",
    "            \n",
    "\n",
    "### Advice for Applying PCA\n",
    "\n",
    "- Supervised Learning speedup\n",
    "    - Training set $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$\n",
    "    - Extract inputs, i.e. generate unlabeled dataset from the training set:  $x^{(1)},x^{(2)},...,x^{(m)} \\in \\mathbb{R}^{10000}$\n",
    "    - Apply PCA to generate $z^{(1)},z^{(2)},...,z^{(m)} \\in \\mathbb{R}^{1000}$\n",
    "    - New Training set: $(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),...,(z^{(m)},y^{(m)})$\n",
    "    \n",
    "- Bad use of PCA = to Prevent Overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
